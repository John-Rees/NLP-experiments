{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA and LDA Topic Modeling for a Corpus of Born Digital Archival Content\n",
    "\n",
    "A factory that takes UTF-8 texts extracted from Word processing documents found in a personal papers archives collection and performs topic modeling exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re #regex module\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import yellowbrick\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import nltk.data\n",
    "from nltk.tokenize import *\n",
    "\n",
    "from nltk.corpus.reader.util import *\n",
    "from nltk.corpus.reader.api import *\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\n",
    "\n",
    "from gensim import matutils, corpora\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import MmCorpus, dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import string\n",
    "#import docx2txt\n",
    "#import docx\n",
    "import codecs\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.float_format = '{:,.10f}'.format\n",
    "plt.rcParams.update({'font.size': 32})\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up workspaces, perform some EDA, and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = '/path/to/utf-8/textfiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a corpus by walking through a directory of files, open each file and read filename and text into a 2 column dataframe\n",
    "corpus = defaultdict(list)\n",
    "for file in Path(corpus_root).iterdir():\n",
    "    with open(file, \"r\") as file_open:\n",
    "        corpus[\"file_name\"].append(file.name)\n",
    "        corpus[\"text\"].append(file_open.read())\n",
    "df = pd.DataFrame(corpus)\n",
    "\n",
    "\n",
    "print(df)\n",
    "#print(corpus['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trim empty text rows \n",
    "nan_value = float(\"NaN\")\n",
    "\n",
    "df.replace(\"\", nan_value, inplace=True)\n",
    "\n",
    "df.dropna(subset = [\"text\"], inplace=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch wordcount for each text\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df[['text','word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Descriptive statistics of word counts\n",
    "df.word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('\\n',' ', regex=True)\n",
    "\n",
    "print (df)\n",
    "                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#add words that aren't in the small NLTK stopwords list\n",
    "#it's big but don't need to worry about it for just a wordcloud\n",
    "#new_stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\", \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\", \"able\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"amongst\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\", \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\", \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"became\", \"become\", \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\", \"brief\", \"briefly\", \"c\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"couldnt\", \"date\", \"different\", \"done\", \"downwards\", \"due\", \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\", \"h\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\", \"however\", \"hundred\", \"id\", \"ie\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"inc\", \"indeed\", \"index\", \"information\", \"instead\", \"invention\", \"inward\", \"itd\", \"it'll\", \"j\", \"k\", \"keep\", \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\", \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\", \"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"one\", \"ones\", \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\", \"overall\", \"owing\", \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"thank\", \"thanks\", \"thanx\", \"thats\", \"that've\", \"thence\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"theyd\", \"theyre\", \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\", \"werent\", \"whatever\", \"what'll\", \"whats\", \"whence\", \"whenever\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\", \"willing\", \"wish\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\", \"x\", \"yes\", \"yet\", \"youd\", \"youre\", \"z\", \"zero\", \"a's\", \"ain't\", \"allow\", \"allows\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"associated\", \"best\", \"better\", \"c'mon\", \"c's\", \"cant\", \"changes\", \"clearly\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"corresponding\", \"course\", \"currently\", \"definitely\", \"described\", \"despite\", \"entirely\", \"exactly\", \"example\", \"going\", \"greetings\", \"hello\", \"help\", \"hopefully\", \"ignored\", \"inasmuch\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"it'd\", \"keep\", \"keeps\", \"novel\", \"presumably\", \"reasonably\", \"second\", \"secondly\", \"sensible\", \"serious\", \"seriously\", \"sure\", \"t's\", \"third\", \"thorough\", \"thoroughly\", \"three\", \"well\", \"wonder\", \"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"co\", \"op\", \"research-articl\", \"pagecount\", \"cit\", \"ibid\", \"les\", \"le\", \"au\", \"que\", \"est\", \"pas\", \"vol\", \"el\", \"los\", \"pp\", \"u201d\", \"well-b\", \"http\", \"volumtype\", \"par\", \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"ac\", \"ad\", \"ae\", \"af\", \"ag\", \"aj\", \"al\", \"an\", \"ao\", \"ap\", \"ar\", \"av\", \"aw\", \"ax\", \"ay\", \"az\", \"b1\", \"b2\", \"b3\", \"ba\", \"bc\", \"bd\", \"be\", \"bi\", \"bj\", \"bk\", \"bl\", \"bn\", \"bp\", \"br\", \"bs\", \"bt\", \"bu\", \"bx\", \"c1\", \"c2\", \"c3\", \"cc\", \"cd\", \"ce\", \"cf\", \"cg\", \"ch\", \"ci\", \"cj\", \"cl\", \"cm\", \"cn\", \"cp\", \"cq\", \"cr\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d2\", \"da\", \"dc\", \"dd\", \"de\", \"df\", \"di\", \"dj\", \"dk\", \"dl\", \"do\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"dx\", \"dy\", \"e2\", \"e3\", \"ea\", \"ec\", \"ed\", \"ee\", \"ef\", \"ei\", \"ej\", \"el\", \"em\", \"en\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"et\", \"eu\", \"ev\", \"ex\", \"ey\", \"f2\", \"fa\", \"fc\", \"ff\", \"fi\", \"fj\", \"fl\", \"fn\", \"fo\", \"fr\", \"fs\", \"ft\", \"fu\", \"fy\", \"ga\", \"ge\", \"gi\", \"gj\", \"gl\", \"go\", \"gr\", \"gs\", \"gy\", \"h2\", \"h3\", \"hh\", \"hi\", \"hj\", \"ho\", \"hr\", \"hs\", \"hu\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ic\", \"ie\", \"ig\", \"ih\", \"ii\", \"ij\", \"il\", \"in\", \"io\", \"ip\", \"iq\", \"ir\", \"iv\", \"ix\", \"iy\", \"iz\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"ke\", \"kg\", \"kj\", \"km\", \"ko\", \"l2\", \"la\", \"lb\", \"lc\", \"lf\", \"lj\", \"ln\", \"lo\", \"lr\", \"ls\", \"lt\", \"m2\", \"ml\", \"mn\", \"mo\", \"ms\", \"mt\", \"mu\", \"n2\", \"nc\", \"nd\", \"ne\", \"ng\", \"ni\", \"nj\", \"nl\", \"nn\", \"nr\", \"ns\", \"nt\", \"ny\", \"oa\", \"ob\", \"oc\", \"od\", \"of\", \"og\", \"oi\", \"oj\", \"ol\", \"om\", \"on\", \"oo\", \"oq\", \"or\", \"os\", \"ot\", \"ou\", \"ow\", \"ox\", \"oz\", \"p1\", \"p2\", \"p3\", \"pc\", \"pd\", \"pe\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"pm\", \"pn\", \"po\", \"pq\", \"pr\", \"ps\", \"pt\", \"pu\", \"py\", \"qj\", \"qu\", \"r2\", \"ra\", \"rc\", \"rd\", \"rf\", \"rh\", \"ri\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"rv\", \"ry\", \"s2\", \"sa\", \"sc\", \"sd\", \"se\", \"sf\", \"si\", \"sj\", \"sl\", \"sm\", \"sn\", \"sp\", \"sq\", \"sr\", \"ss\", \"st\", \"sy\", \"sz\", \"t1\", \"t2\", \"t3\", \"tb\", \"tc\", \"td\", \"te\", \"tf\", \"th\", \"ti\", \"tj\", \"tl\", \"tm\", \"tn\", \"tp\", \"tq\", \"tr\", \"ts\", \"tt\", \"tv\", \"tx\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"uo\", \"ur\", \"ut\", \"va\", \"wa\", \"vd\", \"wi\", \"vj\", \"vo\", \"wo\", \"vq\", \"vt\", \"vu\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y2\", \"yj\", \"yl\", \"yr\", \"ys\", \"yt\", \"zi\", \"zz\"]\n",
    "\n",
    "#mysql's stopwords list\n",
    "new_stopwords = [\"a's\" , \"able\" , \"about\" , \"above\" , \"according\" , \"accordingly\" , \"across\" , \"actually\" , \"after\" , \"afterwards\" , \"again\" , \"against\" , \"ain't\" , \"all\" , \"allow\" , \"allows\" , \"almost\" , \"alone\" , \"along\" , \"already\" , \"also\" , \"although\" , \"always\" , \"am\" , \"among\" , \"amongst\" , \"an\" , \"and\" , \"another\" , \"any\" , \"anybody\" , \"anyhow\" , \"anyone\" , \"anything\" , \"anyway\" , \"anyways\" , \"anywhere\" , \"apart\" , \"appear\" , \"appreciate\" , \"appropriate\" , \"are\" , \"aren't\" , \"around\" , \"as\" , \"aside\" , \"ask\" , \"asking\" , \"associated\" , \"at\" , \"available\" , \"away\" , \"awfully\" , \"be\" , \"became\" , \"because\" , \"become\" , \"becomes\" , \"becoming\" , \"been\" , \"before\" , \"beforehand\" , \"behind\" , \"being\" , \"believe\" , \"below\" , \"beside\" , \"besides\" , \"best\" , \"better\" , \"between\" , \"beyond\" , \"both\" , \"brief\" , \"but\" , \"by\" , \"c'mon\" , \"c's\" , \"came\" , \"can\" , \"can't\" , \"cannot\" , \"cant\" , \"cause\" , \"causes\" , \"certain\" , \"certainly\" , \"changes\" , \"clearly\" , \"co\" , \"com\" , \"come\" , \"comes\" , \"concerning\" , \"consequently\" , \"consider\" , \"considering\" , \"contain\" , \"containing\" , \"contains\" , \"corresponding\" , \"could\" , \"couldn't\" , \"course\" , \"currently\" , \"definitely\" , \"described\" , \"despite\" , \"did\" , \"didn't\" , \"different\" , \"do\" , \"does\" , \"doesn't\" , \"doing\" , \"don't\" , \"done\" , \"down\" , \"downwards\" , \"during\" , \"each\" , \"edu\" , \"eg\" , \"eight\" , \"either\" , \"else\" , \"elsewhere\" , \"enough\" , \"entirely\" , \"especially\" , \"et\" , \"etc\" , \"even\" , \"ever\" , \"every\" , \"everybody\" , \"everyone\" , \"everything\" , \"everywhere\" , \"ex\" , \"exactly\" , \"example\" , \"except\" , \"far\" , \"few\" , \"fifth\" , \"first\" , \"five\" , \"followed\" , \"following\" , \"follows\" , \"for\" , \"former\" , \"formerly\" , \"forth\" , \"four\" , \"from\" , \"further\" , \"furthermore\" , \"get\" , \"gets\" , \"getting\" , \"given\" , \"gives\" , \"go\" , \"goes\" , \"going\" , \"gone\" , \"got\" , \"gotten\" , \"greetings\" , \"had\" , \"hadn't\" , \"happens\" , \"hardly\" , \"has\" , \"hasn't\" , \"have\" , \"haven't\" , \"having\" , \"he\" , \"he's\" , \"hello\" , \"help\" , \"hence\" , \"her\" , \"here\" , \"here's\" , \"hereafter\" , \"hereby\" , \"herein\" , \"hereupon\" , \"hers\" , \"herself\" , \"hi\" , \"him\" , \"himself\" , \"his\" , \"hither\" , \"hopefully\" , \"how\" , \"howbeit\" , \"however\" , \"i'd\" , \"i'll\" , \"i'm\" , \"i've\" , \"ie\" , \"if\" , \"ignored\" , \"immediate\" , \"in\" , \"inasmuch\" , \"inc\" , \"indeed\" , \"indicate\" , \"indicated\" , \"indicates\" , \"inner\" , \"insofar\" , \"instead\" , \"into\" , \"inward\" , \"is\" , \"isn't\" , \"it\" , \"it'd\" , \"it'll\" , \"it's\" , \"its\" , \"itself\" , \"just\" , \"keep\" , \"keeps\" , \"kept\" , \"know\" , \"known\" , \"knows\" , \"last\" , \"lately\" , \"later\" , \"latter\" , \"latterly\" , \"least\" , \"less\" , \"lest\" , \"let\" , \"let's\" , \"like\" , \"liked\" , \"likely\" , \"little\" , \"look\" , \"looking\" , \"looks\" , \"ltd\" , \"mainly\" , \"many\" , \"may\" , \"maybe\" , \"me\" , \"mean\" , \"meanwhile\" , \"merely\" , \"might\" , \"more\" , \"moreover\" , \"most\" , \"mostly\" , \"much\" , \"must\" , \"my\" , \"myself\" , \"name\" , \"namely\" , \"nd\" , \"near\" , \"nearly\" , \"necessary\" , \"need\" , \"needs\" , \"neither\" , \"never\" , \"nevertheless\" , \"new\" , \"next\" , \"nine\" , \"no\" , \"nobody\" , \"non\" , \"none\" , \"noone\" , \"nor\" , \"normally\" , \"not\" , \"nothing\" , \"novel\" , \"now\" , \"nowhere\" , \"obviously\" , \"of\" , \"off\" , \"often\" , \"oh\" , \"ok\" , \"okay\" , \"old\" , \"on\" , \"once\" , \"one\" , \"ones\" , \"only\" , \"onto\" , \"or\" , \"other\" , \"others\" , \"otherwise\" , \"ought\" , \"our\" , \"ours\" , \"ourselves\" , \"out\" , \"outside\" , \"over\" , \"overall\" , \"own\" , \"particular\" , \"particularly\" , \"per\" , \"perhaps\" , \"placed\" , \"please\" , \"plus\" , \"possible\" , \"presumably\" , \"probably\" , \"provides\" , \"que\" , \"quite\" , \"qv\" , \"rather\" , \"rd\" , \"re\" , \"really\" , \"reasonably\" , \"regarding\" , \"regardless\" , \"regards\" , \"relatively\" , \"respectively\" , \"right\" , \"said\" , \"same\" , \"saw\" , \"say\" , \"saying\" , \"says\" , \"second\" , \"secondly\" , \"see\" , \"seeing\" , \"seem\" , \"seemed\" , \"seeming\" , \"seems\" , \"seen\" , \"self\" , \"selves\" , \"sensible\" , \"sent\" , \"serious\" , \"seriously\" , \"seven\" , \"several\" , \"shall\" , \"she\" , \"should\" , \"shouldn't\" , \"since\" , \"six\" , \"so\" , \"some\" , \"somebody\" , \"somehow\" , \"someone\" , \"something\" , \"sometime\" , \"sometimes\" , \"somewhat\" , \"somewhere\" , \"soon\" , \"sorry\" , \"specified\" , \"specify\" , \"specifying\" , \"still\" , \"sub\" , \"such\" , \"sup\" , \"sure\" , \"t's\" , \"take\" , \"taken\" , \"tell\" , \"tends\" , \"th\" , \"than\" , \"thank\" , \"thanks\" , \"thanx\" , \"that\" , \"that's\" , \"thats\" , \"the\" , \"their\" , \"theirs\" , \"them\" , \"themselves\" , \"then\" , \"thence\" , \"there\" , \"there's\" , \"thereafter\" , \"thereby\" , \"therefore\" , \"therein\" , \"theres\" , \"thereupon\" , \"these\" , \"they\" , \"they'd\" , \"they'll\" , \"they're\" , \"they've\" , \"think\" , \"third\" , \"this\" , \"thorough\" , \"thoroughly\" , \"those\" , \"though\" , \"three\" , \"through\" , \"throughout\" , \"thru\" , \"thus\" , \"to\" , \"together\" , \"too\" , \"took\" , \"toward\" , \"towards\" , \"tried\" , \"tries\" , \"truly\" , \"try\" , \"trying\" , \"twice\" , \"two\" , \"un\" , \"under\" , \"unfortunately\" , \"unless\" , \"unlikely\" , \"until\" , \"unto\" , \"up\" , \"upon\" , \"us\" , \"use\" , \"used\" , \"useful\" , \"uses\" , \"using\" , \"usually\" , \"value\" , \"various\" , \"very\" , \"via\" , \"viz\" , \"vs\" , \"want\" , \"wants\" , \"was\" , \"wasn't\" , \"way\" , \"we\" , \"we'd\" , \"we'll\" , \"we're\" , \"we've\" , \"welcome\" , \"well\" , \"went\" , \"were\" , \"weren't\" , \"what\" , \"what's\" , \"whatever\" , \"when\" , \"whence\" , \"whenever\" , \"where\" , \"where's\" , \"whereafter\" , \"whereas\" , \"whereby\" , \"wherein\" , \"whereupon\" , \"wherever\" , \"whether\" , \"which\" , \"while\" , \"whither\" , \"who\" , \"who's\" , \"whoever\" , \"whole\" , \"whom\" , \"whose\" , \"why\" , \"will\" , \"willing\" , \"wish\" , \"with\" , \"within\" , \"without\" , \"won't\" , \"wonder\" , \"would\" , \"wouldn't\" , \"yes\" , \"yet\" , \"you\" , \"you'd\" , \"you'll\" , \"you're\" , \"you've\" , \"your\" , \"yours\" , \"yourself\" , \"yourselves\" , \"zero\"]\n",
    "new_stopwords_list = stop_words.union(new_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Wordcloud function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_wordcloud(text):\n",
    "    \"\"\"Take a set of text, apply Wordcloud and plot the results as a wordcloud visualization \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #stop_words = new_stopwords_list\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stop_words,\n",
    "        max_words=100,\n",
    "        max_font_size=30,\n",
    "        scale=3,\n",
    "        random_state=1)\n",
    "   \n",
    "    wordcloud=wordcloud.generate(str(text))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    #plt.show()\n",
    "    plt.savefig(\"/path/to/savedfile/wordcloud-lda-nltk-stopwords.png\", format=\"png\")\n",
    "\n",
    "show_wordcloud(corpus['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "#vectorizer = CountVectorizer(stop_words=new_stopwords_list)\n",
    "docs       = vectorizer.fit_transform(corpus['text'])\n",
    "features   = vectorizer.get_feature_names()\n",
    "\n",
    "visualizer = FreqDistVisualizer(n=20, features=features, size=(1080, 720)) #default N is 50, can change counts here\n",
    "visualizer.fit(docs)\n",
    "\n",
    "    \n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up texts some more Workspace\n",
    "varmus_files=pd.DataFrame(df, columns=['text'])\n",
    "\n",
    "#remove special characters\n",
    "varmus_files['clean_documents'] = df['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "#remove words have letters less than 3\n",
    "varmus_files['clean_documents'] = varmus_files['clean_documents'].fillna('').apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "\n",
    "#lowercase all characters\n",
    "varmus_files['clean_documents'] = varmus_files['clean_documents'].fillna('').apply(lambda x: x.lower())\n",
    "\n",
    "# Remove punctuation\n",
    "varmus_files['clean_documents'] = varmus_files['clean_documents'].map(lambda x: re.sub(r'[^\\w\\s]', '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "varmus_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the TF-IDF DTM\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             use_idf=True, \n",
    "                             smooth_idf=True)\n",
    "\n",
    "# SVD to reduce dimensionality; n_components defines the number of topics; change this to learn more: \n",
    "# could combine gridsearch to tell us how many to define n_components\n",
    "svd_model = TruncatedSVD(n_components=3,         \n",
    "                         algorithm='randomized',\n",
    "                         n_iter=20)\n",
    "\n",
    "# Using a pipeline for fun to combine tf-idf + SVD fitting against corpus\n",
    "svd_transformer = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model)])\n",
    "svd_matrix = svd_transformer.fit_transform(varmus_files['clean_documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate 3 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's capture the output of our topic model\n",
    "topic_encoded_df = pd.DataFrame(svd_matrix, columns = [\"topic_1\", \"topic_2\", \"topic_3\"])\n",
    "topic_encoded_df[\"documents\"] = varmus_files['clean_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the below - what are some topics?\n",
    "topic_encoded_df.sort_values(by=['topic_1'], ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the below - what are some topics?\n",
    "topic_encoded_df.sort_values(by=['topic_2'], ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the below - what are some topics?\n",
    "topic_encoded_df.sort_values(by=['topic_3'], ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varmus_files_lda = pd.DataFrame(df, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trim empty text rows \n",
    "nan_value = float(\"NaN\")\n",
    "\n",
    "df.replace(\"\", nan_value, inplace=True)\n",
    "\n",
    "df = df.replace('\\n',' ', regex=True)\n",
    "\n",
    "df.dropna(subset = [\"text\"], inplace=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "varmus_files_lda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "varmus_files_lda['processed'] = varmus_files_lda['text'].map(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "varmus_files_lda['processed'] = varmus_files_lda['processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows of papers\n",
    "varmus_files_lda['processed'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize the data to reduce the feature space\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "varmus_files_lda['processed_lemma'] = varmus_files_lda['processed'].map(lambda x: [token.lemma_ for token in nlp(x) if token.lemma_ \n",
    "!= '-PRON-' and token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV'}])\n",
    "\n",
    "# Final cleaning, dropping 2 letter words\n",
    "varmus_files_lda['new_processed_lemma'] = varmus_files_lda['processed_lemma'].map(lambda x: [t for t in x if len(t) > 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varmus_files_lda['new_processed_lemma'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(varmus_files_lda['new_processed_lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(varmus_files_lda['new_processed_lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus from a list of the texts with a large sample n\n",
    "texts = varmus_files_lda.sample(n=200, random_state=36)['new_processed_lemma'].values\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View vectors\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics=9\n",
    "\n",
    "my_lda = LdaModel(corpus, num_topics=n_topics, id2word=dictionary, random_state=24, minimum_probability=0, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can limit what is returned by specifying the number of topics we're interested in and the number of works to return\n",
    "num_topics = 9\n",
    "num_words = 5\n",
    "for ti, topic in enumerate(my_lda.show_topics(num_topics = num_topics, num_words= num_words)):\n",
    "    print(\"Topic: %d\" % (ti))\n",
    "    print (topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate some validation scores using coherence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How similar are topics? Remember cosine similarity. It's built-in to gensim letting us see how similar topics are to each other\n",
    "# how similar are topic 0 and\n",
    "matutils.cossim(my_lda.get_topic_terms(0), my_lda.get_topic_terms(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=my_lda, texts=varmus_files_lda['new_processed_lemma'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the coherence score - high or low? Generate a coherence model to find the sweet spot for number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a coherence model to find the sweet spot for number of topics. Computation will take a while\n",
    "max_topics = 30\n",
    "coh_list = []\n",
    "for n_topics in range(3,max_topics+1):\n",
    "    # Train the model on the corpus\n",
    "    my_lda = LdaModel(corpus, num_topics=n_topics, id2word=dictionary, random_state=42, alpha=0.1, eta=0.9, passes=10)\n",
    "    # Estimate coherence\n",
    "    cm = CoherenceModel(model=my_lda, texts=texts, dictionary=dictionary, coherence='c_v', topn=20)\n",
    "    coherence = cm.get_coherence_per_topic() # get coherence value\n",
    "    coh_list.append(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coherence scores:\n",
    "coh_means = np.array([np.mean(l) for l in coh_list])\n",
    "coh_stds = np.array([np.std(l) for l in coh_list])\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "plt.xticks(np.arange(3, max_topics+1, 3.0));\n",
    "plt.plot(range(3,max_topics+1), coh_means);\n",
    "plt.fill_between(range(3,max_topics+1), coh_means-coh_stds, coh_means+coh_stds, color='g', alpha=0.1);\n",
    "plt.vlines([15, 17], 0.4, 0.6, color='red', linestyles='dashed',  linewidth=1);\n",
    "plt.hlines([0.253], 3, max_topics, color='black', linestyles='dotted',  linewidth=0.5);\n",
    "plt.savefig(\"/path/to/savedfile/coherence-plot.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics=16\n",
    "\n",
    "my_lda = LdaModel(corpus, num_topics=n_topics, id2word=dictionary, random_state=42, minimum_probability=0, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 16\n",
    "num_words = 8\n",
    "for ti, topic in enumerate(my_lda.show_topics(num_topics = num_topics, num_words= num_words)):\n",
    "    print(\"Topic: %d\" % (ti))\n",
    "    print (topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How similar are topics? Remember cosine similarity from above. It's built-in to gensim letting us see how similar topics are to each other\n",
    "# how similar are topic 0 and 1 62%\n",
    "matutils.cossim(my_lda.get_topic_terms(0), my_lda.get_topic_terms(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=my_lda, texts=varmus_files_lda['new_processed_lemma'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Topic Model with n-topics from coherence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading our model with 16 topics\n",
    "n_topics=16\n",
    "#n_topics=6\n",
    "n_top_words = 30\n",
    "my_lda = LdaModel(corpus, num_topics=n_topics, id2word=dictionary, random_state=42, minimum_probability=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim_models.prepare(my_lda, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to interactive stand-alone html page\n",
    "p = pyLDAvis.gensim_models.prepare(my_lda, corpus, dictionary)\n",
    "pyLDAvis.save_html(p, '/path/to/savedfile/varmus-lda-16-topics.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
