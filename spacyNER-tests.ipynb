{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Extraction from a Corpus of Born Digital Archival Content\n",
    "\n",
    "A factory that takes UTF-8 texts extracted from Word processing documents found in a personal papers archives collection and generates some NERs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "import re #regex module\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from gensim import matutils, corpora\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import MmCorpus, dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#nlp = en_core_web_sm.load()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 5000000 #spacy default has 1M character limit\n",
    "\n",
    "pd.options.display.float_format = '{:,.8f}'.format\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up workspaces and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = '/path/to/utf-8/textfiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a corpus by walking through a directory of files, open each file and read filename and text into a 2 column dataframe\n",
    "corpus = defaultdict(list)\n",
    "for file in Path(corpus_root).iterdir():\n",
    "    with open(file, \"r\") as file_open:\n",
    "        corpus[\"file_name\"].append(file.name)\n",
    "        corpus[\"text\"].append(file_open.read())\n",
    "df = pd.DataFrame(corpus)\n",
    "\n",
    "\n",
    "print(df)\n",
    "#print(corpus['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trim empty text rows \n",
    "nan_value = float(\"NaN\")\n",
    "\n",
    "df.replace(\"\", nan_value, inplace=True)\n",
    "\n",
    "df = df.replace('\\n',' ', regex=True)\n",
    "\n",
    "df.dropna(subset = [\"text\"], inplace=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn text files into a string and count the number of characters into a new column\n",
    "df['charcount'] = df['text'].str.len()\n",
    "\n",
    "#sum the new column and put it into a variable\n",
    "total = df['charcount'].sum()\n",
    "\n",
    "#print the variable\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization factory and extract entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a variable called 'tokens' that contains all the words in the 'text' column of the df dataframe\n",
    "\n",
    "tokens = nlp(''.join(str(df.text.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract most common token\n",
    "items = [x.text for x in tokens.ents]\n",
    "Counter(items).most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize entities with spaCY/displacy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(tokens, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"ents\": [\"PERSON\"]}\n",
    "displacy.render(tokens, style=\"ent\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER People plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_list = []\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        person_list.append(ent.text)\n",
    "        \n",
    "person_counts = Counter(person_list).most_common(20)\n",
    "df_person = pd.DataFrame(person_counts, columns =['text', 'count'])\n",
    "#df_person.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person.plot.barh(x='text', y='count', title=\"Top 20 Personal Names\", figsize=(10,8)).invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Organization Names plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_list = []\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ == 'ORG':\n",
    "        org_list.append(ent.text)\n",
    "        \n",
    "org_counts = Counter(org_list).most_common(20)\n",
    "df_org = pd.DataFrame(org_counts, columns =['text', 'count'])\n",
    "#df_org.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org.plot.barh(x='text', y='count', title=\"Top 20 Organizational Names\", figsize=(10,8)).invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Geographic Names plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geog_list = []\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ == 'GPE':\n",
    "        geog_list.append(ent.text)\n",
    "        \n",
    "geog_counts = Counter(geog_list).most_common(20)\n",
    "df_geog = pd.DataFrame(geog_counts, columns =['text', 'count'])\n",
    "#df_geog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geog.plot.barh(x='text', y='count', title=\"Top 20 Geographic Terms\", figsize=(10,8)).invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Products plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = []\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ == 'PRODUCT':\n",
    "        product_list.append(ent.text)\n",
    "        \n",
    "product_counts = Counter(product_list).most_common(20)\n",
    "df_product = pd.DataFrame(product_counts, columns =['text', 'count'])\n",
    "#df_product.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product.plot.barh(x='text', y='count', title=\"Top 20 Products\", figsize=(10,8)).invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Noun/pronoun plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_list = []\n",
    "\n",
    "for x in tokens :\n",
    "    if x.pos_ == \"NOUN\" or x.pos_ == \"PROPN\":\n",
    "        noun_list.append(x.text)\n",
    "\n",
    "noun_counts = Counter(noun_list).most_common(20)\n",
    "df_noun = pd.DataFrame(noun_counts, columns =['text', 'count'])\n",
    "#df_noun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noun.plot.barh(x='text', y='count', title=\"Top 20 Nouns and Proper Nouns\", figsize=(10,8)).invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Dates plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_list = []\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ == 'DATE':\n",
    "        dates_list.append(ent.text)\n",
    "        \n",
    "dates_counts = Counter(dates_list).most_common(20)\n",
    "df_dates = pd.DataFrame(dates_counts, columns =['text', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dates.plot.barh(x='text', y='count', title=\"Top 20 Dates\", figsize=(10,8)).invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting and identifying ents, trying to get them all into a list so we can see which ones are worth plotting\n",
    "Move this up to EDA eventually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in tokens.ents:\n",
    "    print(ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in tokens.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([ent for ent in tokens.ents if ent.label_=='PERSON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([ent for ent in tokens.ents if ent.label_=='GPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([ent for ent in tokens.ents if ent.label_=='ORG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([ent for ent in tokens.ents if ent.label_=='DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in tokens.ents:\n",
    "    table = \n",
    "    people = len([ent for ent in tokens.ents if ent.label_=='PERSON'])\n",
    "    org = len([ent for ent in tokens.ents if ent.label_=='ORG'])\n",
    "    gpe = len([ent for ent in tokens.ents if ent.label_=='GPE'])\n",
    "    date = len([ent for ent in tokens.ents if ent.label_=='DATE'])\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
